[
  {
    "activity_json": [
      {
        "activityType": "youtubevideo", 
        "title": "Lecture 2: The Neural as an Universal Approximator", 
        "videoResources": "http://ec2-13-233-148-70.ap-south-1.compute.amazonaws.com/submissions/intro_to_deep_learning/Neural%20Networks%20What%20can%20a%20network%20represent/attachments/lec2.universal.pdf", 
        "videoURL": "4PsbO9LbEHI"
      }, 
      {
        "URL": "piazza.com", 
        "activityType": "discussion", 
        "title": "Discussion forum"
      }
    ], 
    "activity_name": "Lecture 2: The Neural as an Universal Approximator", 
    "id": 2, 
    "module_id": 2
  }, 
  {
    "activity_json": [
      {
        "activityType": "assignment", 
        "questions": [
          {
            "questionText": [
              {
                "text": "Please find the pdf and  zip file"
              }, 
              {
                "attachment": [
                  "http://ec2-13-233-148-70.ap-south-1.compute.amazonaws.com/submissions/intro_to_deep_learning/Neural%20Networks%20What%20can%20a%20network%20represent/attachments/rec2.pdf"
                ]
              }, 
              {
                "attachment": [
                  "http://ec2-13-233-148-70.ap-south-1.compute.amazonaws.com/submissions/intro_to_deep_learning/Neural%20Networks%20What%20can%20a%20network%20represent/attachments/recitation-2.zip"
                ]
              }
            ], 
            "questionType": "filesubmission"
          }
        ], 
        "title": "Assignment"
      }
    ], 
    "activity_name": "Assignment", 
    "id": 36, 
    "module_id": 2
  }, 
  {
    "activity_json": [
      {
        "activityType": "quiz", 
        "questions": [
          {
            "correct_feedback": "Correct, the \"cache\" records values from the forward propagation units and sends it to the backward propagation units because it is needed to compute the chain rule derivatives.", 
            "max_marks": "1", 
            "options": [
              {
                "correct": false, 
                "option": "It is used to keep track of the hyperparameters that we are searching over, to speed up computation."
              }, 
              {
                "correct": true, 
                "option": "We use it to pass variables computed during forward propagation to the corresponding backward propagation step. It contains useful values for backward propagation to compute derivatives."
              }, 
              {
                "correct": false, 
                "option": "We use it to pass variables computed during backward propagation to the corresponding forward propagation step. It contains useful values for forward propagation to compute activations."
              }, 
              {
                "correct": false, 
                "option": "It is used to cache the intermediate values of the cost function during training."
              }
            ], 
            "questionText": [
              {
                "text": "What is the \"cache\" used for in our implementation of forward propagation and backward propagation?"
              }
            ], 
            "questionType": "mcq", 
            "wrong_feedback": "Incorrect"
          }, 
          {
            "correct_feedback": "correct", 
            "max_marks": "1", 
            "options": [
              {
                "correct": true, 
                "option": "activation values a<sup>[l]</sup>"
              }, 
              {
                "correct": true, 
                "option": "weight matrices W<sup>[l]</sup>"
              }, 
              {
                "correct": true, 
                "option": "number of layers L in the neural network"
              }, 
              {
                "correct": true, 
                "option": "learning rate \u03b1"
              }, 
              {
                "correct": true, 
                "option": "number of iterations "
              }, 
              {
                "correct": true, 
                "option": "size of the hidden layers n<sup>[l]</sup>"
              }, 
              {
                "correct": true, 
                "option": "bias vectors b<sup>[l]</sup>"
              }
            ], 
            "questionText": [
              {
                "text": "Among the following, which ones are \"hyperparameters\"? (Check all that apply.)"
              }
            ], 
            "questionType": "checkbox", 
            "wrong_feedback": "Incorrect"
          }, 
          {
            "correct_feedback": "correct", 
            "max_marks": "1", 
            "options": [
              {
                "correct": true, 
                "option": "The deeper layers of a neural network are typically computing more complex features of the input than the earlier layers."
              }, 
              {
                "correct": false, 
                "option": "The earlier layers of a neural network are typically computing more complex features of the input than the deeper layers."
              }
            ], 
            "questionText": [
              {
                "text": "Which of the following statements is true?"
              }
            ], 
            "questionType": "mcq", 
            "wrong_feedback": "Incorrect"
          }, 
          {
            "correct_feedback": "Correct Forward propagation propagates the input through the layers, although for shallow networks we may just write all the lines ( a <sup>[2]</sup>= g <sup>[2]</sup>( z sup[2]</sup>, ) z <sup>[2]</sup>= W <sup>[2][1]</sup>a<sup>[2]</sup>+ b, ...)in a deeper network, we cannot avoid a for loop iterating over the layers: (<sup>[l]</sup>a= g<sup>[l]</sup>( z<sup>[l]</sup>,) z<sup>[l]</sup>= W<sup>[l][l\u22121]</sup>a<sup>[l]</sup>+ b, ...).", 
            "max_marks": "1", 
            "options": [
              {
                "correct": false, 
                "option": "True"
              }, 
              {
                "correct": true, 
                "option": "False"
              }
            ], 
            "questionText": [
              {
                "text": "Vectorization allows you to compute forward propagation in an L -layer neural network without an explicit for-loop (or any other explicit iterative loop) over the layers l=1, 2, ...,L. True/False?"
              }
            ], 
            "questionType": "mcq", 
            "wrong_feedback": "Incorrect"
          }, 
          {
            "correct_feedback": "Correct", 
            "max_marks": "1", 
            "options": [
              {
                "correct": false, 
                "option": "Figure-1"
              }, 
              {
                "correct": false, 
                "option": "Figure 2:"
              }, 
              {
                "correct": false, 
                "option": "Figure 3:"
              }, 
              {
                "correct": true, 
                "option": "Figure 4:"
              }
            ], 
            "questionText": [
              {
                "text": "Assume we store the values for n [l] in an array called layers, as follows: layer_dims = [ n x , 4,3,2,1]. So layer 1 has four hidden units, layer 2 has 3 hidden units and so on. Which of the following for-loops will allow you to initialize the parameters for the model?"
              }, 
              {
                "image": {
                  "imageName": "Figure-1", 
                  "imageSRC": "http://ec2-13-233-148-70.ap-south-1.compute.amazonaws.com/content/intro_to_deep_learning/Neural%20Networks%20What%20can%20a%20network%20represent/attachments/image-1.png"
                }
              }, 
              {
                "image": {
                  "imageName": "Figure-2", 
                  "imageSRC": "http://ec2-13-233-148-70.ap-south-1.compute.amazonaws.com/content/intro_to_deep_learning/Neural%20Networks%20What%20can%20a%20network%20represent/attachments/image-2.png"
                }
              }, 
              {
                "image": {
                  "imageName": "Figure-3", 
                  "imageSRC": "http://ec2-13-233-148-70.ap-south-1.compute.amazonaws.com/content/intro_to_deep_learning/Neural%20Networks%20What%20can%20a%20network%20represent/attachments/image-3.png"
                }
              }, 
              {
                "image": {
                  "imageName": "Figure-4", 
                  "imageSRC": "http://ec2-13-233-148-70.ap-south-1.compute.amazonaws.com/content/intro_to_deep_learning/Neural%20Networks%20What%20can%20a%20network%20represent/attachments/image-4.png"
                }
              }
            ], 
            "questionType": "mcq", 
            "wrong_feedback": "Incorrect"
          }, 
          {
            "correct_feedback": "Correct Yes. As seen in lecture, the number of layers is counted as the number of hidden layers + 1. The input and output layers are not counted as hidden layers.", 
            "max_marks": "1", 
            "options": [
              {
                "correct": true, 
                "option": "The number of layers L is 4. The number of hidden layers is 3."
              }, 
              {
                "correct": false, 
                "option": "The number of layers L is 3. The number of hidden layers is 3."
              }, 
              {
                "correct": false, 
                "option": "The number of layers L is 4. The number of hidden layers is 4."
              }, 
              {
                "correct": false, 
                "option": "The number of layers L is 5. The number of hidden layers is 4."
              }
            ], 
            "questionText": [
              {
                "text": "Consider the following neural network."
              }, 
              {
                "image": {
                  "imageName": "", 
                  "imageSRC": "http://ec2-13-233-148-70.ap-south-1.compute.amazonaws.com/content/intro_to_deep_learning/Neural%20Networks%20What%20can%20a%20network%20represent/attachments/image-5.png"
                }
              }, 
              {
                "text": "How many layers does this network have?"
              }
            ], 
            "questionType": "mcq", 
            "wrong_feedback": "Incorrect"
          }, 
          {
            "correct_feedback": "Yes, as you've seen in the week 3 each activation has a di erent derivative. Thus, during backpropagation you need to know which activation was used in the forward propagation to be able to compute the correct derivative.", 
            "max_marks": "1", 
            "options": [
              {
                "correct": true, 
                "option": "True"
              }, 
              {
                "correct": false, 
                "option": "False"
              }
            ], 
            "questionText": [
              {
                "text": "During forward propagation, in the forward function for a layer l you need to know what is the activation function in a layer Networks (Sigmoid, tanh, ReLU, etc.). During backpropagation, the corresponding backward function also needs to know what is the activation function for layer l , since the gradient depends on it. True/False?"
              }
            ], 
            "questionType": "mcq", 
            "wrong_feedback": "Incorrect"
          }, 
          {
            "correct_feedback": "Correct.", 
            "max_marks": "1", 
            "options": [
              {
                "correct": true, 
                "option": "True"
              }, 
              {
                "correct": false, 
                "option": "False"
              }
            ], 
            "questionText": [
              {
                "text": "There are certain functions with the following properties: <br><br>(i) To compute the function using a shallow network circuit, you will need a large network (where we measure size by the number of logic gates in the network), but (ii) To compute it using a deep network circuit, you need only an exponentially smaller network. True/False?"
              }
            ], 
            "questionType": "checkbox", 
            "wrong_feedback": "Incorrect"
          }, 
          {
            "correct_feedback": "Correct", 
            "max_marks": "1", 
            "options": [
              {
                "correct": true, 
                "option": "W<sup>[1]</sup> will have shape (4, 4)"
              }, 
              {
                "correct": true, 
                "option": "b<sup>[1]</sup> will have shape (4, 1)"
              }, 
              {
                "correct": true, 
                "option": "W<sup>[1]</sup> will have shape (3, 4)"
              }, 
              {
                "correct": true, 
                "option": "b<sup>[1]</sup> will have shape (3, 1)"
              }, 
              {
                "correct": true, 
                "option": "W<sup>[2]</sup> will have shape (3, 4)"
              }, 
              {
                "correct": true, 
                "option": "b<sup>[2]</sup> will have shape (1, 1)"
              }, 
              {
                "correct": true, 
                "option": "W<sup>[2]</sup> will have shape (3, 1)"
              }, 
              {
                "correct": true, 
                "option": "b<sup>[2]</sup> will have shape (3, 1)"
              }, 
              {
                "correct": true, 
                "option": "W<sup>[3]</sup> will have shape (3, 1)"
              }, 
              {
                "correct": true, 
                "option": "b<sup>[3]</sup> will have shape (1, 1)"
              }, 
              {
                "correct": true, 
                "option": "W<sup>[3]</sup> will have shape (1, 3)"
              }, 
              {
                "correct": true, 
                "option": "b<sup>[3]</sup> will have shape (3, 1)"
              }
            ], 
            "questionText": [
              {
                "text": "Consider the following 2 hidden layer neural network:"
              }, 
              {
                "image": {
                  "imageName": "", 
                  "imageSRC": "http://ec2-13-233-148-70.ap-south-1.compute.amazonaws.com/content/intro_to_deep_learning/Neural%20Networks%20What%20can%20a%20network%20represent/attachments/image-6.png"
                }
              }, 
              {
                "text": "Which of the following statements are True? (Check all that apply)."
              }
            ], 
            "questionType": "checkbox", 
            "wrong_feedback": "Incorrect"
          }, 
          {
            "correct_feedback": "Correct. True", 
            "max_marks": "1", 
            "options": [
              {
                "correct": false, 
                "option": "W<sup>[l]</sup> have shape (n<sup>[l-1]</sup>, n<sup>[l]</sup>)"
              }, 
              {
                "correct": false, 
                "option": "W<sup>[l]</sup> have shape (n<sup>[l+1]</sup>, n<sup>[l]</sup>)"
              }, 
              {
                "correct": true, 
                "option": "W<sup>[l]</sup> have shape (n<sup>[l]</sup>, n<sup>[l-1]</sup>)"
              }, 
              {
                "correct": false, 
                "option": "W<sup>[l]</sup> have shape (n<sup>[l]</sup>, n<sup>[l+1]</sup>)"
              }
            ], 
            "questionText": [
              {
                "text": "Whereas the previous question used a speci c network, in the general case what is the dimension of W^{[l]}, the weight matrix associated with layer l ?"
              }
            ], 
            "questionType": "mcq", 
            "wrong_feedback": "Incorrect"
          }
        ], 
        "title": "Quiz Msniksn"
      }
    ], 
    "activity_name": "Quiz", 
    "id": 49, 
    "module_id": 2
  }
]
